{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Dimensionality Reduction and Clustering\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will explore Dimensionality Reduction and to a lesser extent Clustering. \n",
    "\n",
    "We will use the MNIST data set, a standard testbed for simple machine learning tasks, which is described in the wiki: https://en.wikipedia.org/wiki/MNIST_database. It comprises a training set of 60,000 handwritten images, and a test set of 10,000. Each data point consists of a 24x24 pixel image, which represents one of 10 handwritten numbers. It is often used for classification purposes, however we will apply different dimensionality and clustering methods to the data set.\n",
    "\n",
    "Initially we will be applying dimensionality reduction for visualisation purposes, and we will begin by applying Principal Components Analysis (PCA): both using Scikit-Learn and a version coded by hand. Following this we will look at using Linear Discriminant Analysis as well as $t$-SNE.\n",
    "\n",
    "Next we will examine different uses of dimensionality reduction, such as: Increasing the training speed of a logistic classifier; improving predictive performance on noisy images; and reconstructing de-noised images.\n",
    "\n",
    "Finally we will introduce clustering to aid classification accuracy when we have only a few labelled training examples.\n",
    "\n",
    "Dimensionality reduction and Clustering are both reasonably large areas within Machine Learning and we have merely scratched the surface of these fields of study. The goal of the laboratory is to come away with an understanding of some application areas and be able to implement the simpler ones yourselves.\n",
    "\n",
    "Some of the code in this notebook originally comes from code accompanying 'Hands-On Machine Learning with Scikit-Learn & Tensorflow', by A. Geron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    "- The structure of the code is given to you and you will need to fill in the parts corresponding to each question. \n",
    "- Do not modify/erase other parts of the code if you have not been given specific instructions to do so.\n",
    "- When you are asked to insert code, do so between the areas which begin:\n",
    "  \n",
    "  `##########################################################`\n",
    "  \n",
    "  `# TO_DO`\n",
    "  \n",
    "  `# [your code here]`\n",
    "   \n",
    "   And which end:\n",
    "   \n",
    "  `# /TO_DO\n",
    "   ##########################################################`\n",
    "\n",
    "\n",
    "- When you are asked to comment on the results you should give clear and comprehensible explanations. Write the comments in a 'Code Cell' with a sign `#` at the beginning of each row, and in the areas which begin:\n",
    "\n",
    "  `# [INSERT YOUR ANSWER HERE]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Please do not change the cell below, you will see a number of imports. All these packages are relevant for the assignment and it is important that you get used to them. You can find more information about them in their respective documentation. As usual Numpy, Pandas, and Scikit-Learn will be used heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Python libraries for data and visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster  import KMeans\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We will begin by loading the MNIST data set, defining some useful helper functions, and plotting some sample training digits from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "#Import our train/ test data plus a few helper functions.\n",
    "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = mnist.load_data()\n",
    "X_train, y_train, X_test, y_test = X_train_orig, y_train_orig, X_test_orig, y_test_orig\n",
    "\n",
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(28, 28),cmap = plt.cm.gray, interpolation='nearest',\n",
    "                  clim=(0, 255))\n",
    "                \n",
    "def plot_2d(projected, y_train, title):\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    p = ax.scatter(projected[:, 0], projected[:, 1],\n",
    "                c=y_train, edgecolor='none', alpha=0.5,\n",
    "                  cmap='tab10')\n",
    "    ax.set_xlabel('component 1')\n",
    "    ax.set_ylabel('component 2')\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(p);\n",
    "    \n",
    "def print_results(y_pred, y_test):\n",
    "    mat = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "    plt.xlabel('true label')\n",
    "    plt.ylabel('predicted label');\n",
    "\n",
    "    print(\"Accuracy on test set: \", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "plot_digits(X_train[:40])\n",
    "num_examples = 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test data\n",
    "\n",
    "__Task:__  \n",
    "Take the first 7000 training examples and use a scaler to scale the data.  \n",
    "Each image is originally in a 24x24 numpy array, so please flatten the data, and be sure to fit the scaler on only the training data.  \n",
    "Please keep the variable names the same, that is: `X_train`, `y_train`, `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction for Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis:\n",
    "\n",
    "We will now use Scikit-Learn to project the data to 2-dimensions.  \n",
    "If you have set up the data correctly then the training data will have shape (7000, 784), the projected data will have shape (7000,2), and the explained variance will be around 10.6%.  \n",
    "Be sure to understand the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. PCA expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#PLEASE DO NOT CHANGE THIS CELL\u001b[39;00m\n\u001b[32m      2\u001b[39m pca = PCA(n_components=\u001b[32m2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m projected = pca.fit_transform(X_train)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining data shape :\u001b[39m\u001b[33m\"\u001b[39m, X_train.shape)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProjected data shape :\u001b[39m\u001b[33m\"\u001b[39m, projected.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlb/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = f(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs)\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlb/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlb/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:468\u001b[39m, in \u001b[36mPCA.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    447\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[32m    448\u001b[39m \n\u001b[32m    449\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    466\u001b[39m \u001b[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     U, S, _, X, x_is_centered, xp = \u001b[38;5;28mself\u001b[39m._fit(X)\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    470\u001b[39m         U = U[:, : \u001b[38;5;28mself\u001b[39m.n_components_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlb/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:505\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    496\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPCA with svd_solver=\u001b[39m\u001b[33m'\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not supported for Array API inputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    497\u001b[39m     )\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m X = validate_data(\n\u001b[32m    506\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    507\u001b[39m     X,\n\u001b[32m    508\u001b[39m     dtype=[xp.float64, xp.float32],\n\u001b[32m    509\u001b[39m     force_writeable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    510\u001b[39m     accept_sparse=(\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    511\u001b[39m     ensure_2d=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    512\u001b[39m     copy=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    513\u001b[39m )\n\u001b[32m    514\u001b[39m \u001b[38;5;28mself\u001b[39m._fit_svd_solver = \u001b[38;5;28mself\u001b[39m.svd_solver\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlb/lib/python3.11/site-packages/sklearn/utils/validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = check_array(X, input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m, **check_params)\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlb/lib/python3.11/site-packages/sklearn/utils/validation.py:1101\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1096\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1097\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1098\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1099\u001b[39m     )\n\u001b[32m   1100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array.ndim >= \u001b[32m3\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m   1107\u001b[39m     _assert_all_finite(\n\u001b[32m   1108\u001b[39m         array,\n\u001b[32m   1109\u001b[39m         input_name=input_name,\n\u001b[32m   1110\u001b[39m         estimator_name=estimator_name,\n\u001b[32m   1111\u001b[39m         allow_nan=ensure_all_finite == \u001b[33m\"\u001b[39m\u001b[33mallow-nan\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1112\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found array with dim 3. PCA expected <= 2."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "pca = PCA(n_components=2)\n",
    "projected = pca.fit_transform(X_train)\n",
    "\n",
    "print(\"Training data shape :\", X_train.shape)\n",
    "print(\"Projected data shape :\", projected.shape)\n",
    "print(\"Explained variance :\", np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Projected Data:\n",
    "\n",
    "We now visualise the 7000 training images in 2-dimensions.  \n",
    "Notice that similarly classified images tend to be somewhat clustered together (remember that PCA is an unsupervised technique and does not know the training target labels).  \n",
    "Also note that there is considerable overlap in the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plot_2d(projected, y_train[:num_examples], 'Principal Components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Explained Variance by the Number of Components:\n",
    "\n",
    "The chart below shows the cumulative explained variance as we increase the number of components.  \n",
    "With 784 components we can explain all of the variance (why?), meanwhile projecting into 2 dimensions only explains just over 10% of the variance.\n",
    "\n",
    "__Question:__  \n",
    "How would you go about deciding a good number of components to use in the trade-off between good explainability of the data and a lower dimensional representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "pca = PCA().fit(X_train)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "plt.title('Explained variance as components increase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA by Hand:\n",
    "\n",
    "We are now going to try implementing PCA (and the chart above) by hand:\n",
    "\n",
    "__Task:__  \n",
    "1) Without using the `np.cov` function please create a covariance matrix of the training data, (call this `cov_mat`). The shape of this matrix should be 784x784 (why?)  \n",
    "2) Using `np.linalg.eig` or otherwise, please find the eigenvalues and associated eigenvectors of the data. Store them in numpy arrays named `eig_vals`, `eig_vecs`. `eig_vals` should have shape (784,) and `eig_vecs` should have shape (784,784).  \n",
    "3) Project the data onto 2-dimensions, using the 2 highest eigenvalues and associated eigenvectors, by creating a projection matrix called `matrix_w` (What shape should `matrix_w` be?)  \n",
    "4) Please create a variable, `Y`, by applying the projection matrix to your training data (`Y` should have shape (7000,2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "exp_var = np.sum(eig_vals[:2])/np.sum(eig_vals)\n",
    "\n",
    "print(\"X_train shape :\", X_train.shape)\n",
    "print(\"Covariance matrix shape :\", cov_mat.shape)\n",
    "print(\"Eigenvalues shape :\", eig_vals.shape)\n",
    "print(\"Eigenvectors shape :\",eig_vecs.shape)\n",
    "print(\"Projection matrix shape :\",matrix_w.shape)\n",
    "print(\"Projected data shape:\",Y.shape)\n",
    "print(\"Explained variance: \",exp_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Projected Data Using Your Hand-Coded Version:\n",
    "\n",
    "If you have coded the above correctly then the chart below should match the projection from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plot_2d(-Y, y_train[:num_examples],'Principal Components by hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "We will now apply a similar projection and visualisation with Linear Discriminant Analysis. \n",
    "\n",
    "__Task:__  \n",
    "1) Research Linear Discriminant Analysis and understand key differences with the approach of PCA.  \n",
    "2) Create a variable, `X_lda_sklearn`, which contains the 7000 training examples projected to 2-dimensions using `LDA` in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the LDA Projected Data:\n",
    "\n",
    "Again we show the visualisation of the projected data.  \n",
    "\n",
    "This time you should see a far greater separation of the clusters compared to PCA.\n",
    "\n",
    "__Question:__   \n",
    "Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plot_2d(X_lda_sklearn, y_train[:num_examples],'LDA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $t$-SNE\n",
    "\n",
    "We will now apply $t$-SNE for our final visualisation. We return to applying dimensionality reduction but without knowing the training target labels.\n",
    "\n",
    "__Task:__  \n",
    "1) Look up $t$-SNE and understand at a high level what it attempts to do.   \n",
    "2) To reduce the amount of computation, first reduce the training data to 50 dimensions using PCA.  \n",
    "3) Fit the data using `TSNE`. Use the following paramaters, `n_components=2`, `verbose=1`, `perplexity=40`, `n_iter=300`, `random_state=42`.   \n",
    "4) Store the dimensionality-reduced data in a variable called `X_reduced_tsne`. We will use this variable to plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the $t$-SNE Projected Data:\n",
    "\n",
    "You should see that $t$-SNE has projected the data, clustering images with the same label, but with a better separation between images with different labels (than for example PCA).  \n",
    "Remember that $t$-SNE is not told the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plot_2d(X_reduced_tsne, y_train[:num_examples],'t-SNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction to Reduce Computational Load\n",
    "\n",
    "We will now briefly examine dimensionality reduction to improve the training speed of a simple machine learning algorithm.  \n",
    "We will be applying Logistic Regression to classify the MNIST data.\n",
    "\n",
    "__Task:__  \n",
    "1) Create a Logistic Regression object from Scikit-Learn. Remember that this is a multi-class regression. Set `solver=\"lbfgs\"` (a choice of optimiser to speed things up), and set `random_state=42`.  \n",
    "2) Fit your model to the training data and time how long it took to do so. Store this time in a variable called `training_time`.  \n",
    "3) Apply your model to the test data, `X_test`, and create a set of predictions on the unseen test set. Store these predictions in a variable called `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression Performance:\n",
    "In the cell below the accuracy, training time, and a confusion matrix of the results are displayed.  \n",
    "You should have an accuracy of close to 89%. If you did not get this result, then things to check inlcude: Did you use the 7000 training examples? Were both the training and test data standardised? Did you apply the correct model?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "print_results(y_pred, y_test)\n",
    "print(\"Training took {:.2f}s\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Using PCA:\n",
    "\n",
    "__Task:__  \n",
    "We will now do exactly the same thing: We will produce two variables `y_pred` and `training time`.  \n",
    "However this time we would like you to apply a single preprocessing step:  \n",
    "1) Pre-process the training data using principal components. Specifically, reduce the dimensionality of the training set such that 95% of the variance of the data remains explained (Note that there is a shortcut to do this using the `n_compoonents` parameter).  \n",
    "2) Now repeat the earlier steps, but this time train your model on the reduced dimensionality training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Results:\n",
    "In the cell below the accuracy, training time, and a confusion matrix of the results are displayed.  \n",
    "You should find that we achieve almost exactly the same accuracy on the test set, but we halved the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "print_results(y_pred, y_test)\n",
    "print(\"Training took {:.2f}s\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Applied to Noisy Data\n",
    "\n",
    "We will now apply dimensionality reduction (again PCA) to the problem of noisy data.  \n",
    "In the cell below we have reloaded our training data, but this time have added noise. The visualisation of some of the now noisy training data images is given below.\n",
    "\n",
    "Our variables are now `noisy_train`, `noisy_test`, with `y_train` and `y_test` remaining the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "X_train, y_train, X_test, y_test = X_train_orig, y_train_orig, X_test_orig, y_test_orig\n",
    "X_train = X_train[:num_examples,:]\n",
    "y_train = y_train[:num_examples]\n",
    "\n",
    "np.random.seed(42)\n",
    "noisy_train = np.random.normal(X_train, 100)\n",
    "noisy_test = np.random.normal(X_test, 100)\n",
    "plot_digits(noisy_train[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression Performance - Noisy Data:\n",
    "\n",
    "__Task:__  \n",
    "1) Fit a Logistic Regression model to the `noisy_train data`.  \n",
    "2) Create a set of predictions, again stored in `y_pred` on the test set, `noisy_test`.  \n",
    "(Please check the dimensions of the inputs and reshape if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Results - Noisy Data:\n",
    "We have given the accuracy on the test set as well as the confusion matrix.   \n",
    "You should see that the prediction accuracy has deteriorated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "print_results(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Using PCA - Noisy data:\n",
    "\n",
    "__Task:__   \n",
    "Please perform the same exercise but this time apply principal components to the training data as a pre-processing step and train the model on the reduced dimensionality data.  \n",
    "Set `n_components = 0.25`.   \n",
    "Store the predictions on the test set in `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Results - Noisy Data:\n",
    "\n",
    "Again we have given the accuracy on the test set as well as the confusion matrix.   \n",
    "\n",
    "__Questions:__  \n",
    "1) Why do you think the performance is improved by preprocessing using PCA in this particular example?  \n",
    "2) Do you think this is usually the case, or that indeed performance could have actually deteriorated in some cases?  \n",
    "3) How would you go about deciding whether this might be sensible for your particular training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "print_results(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Via Reconstruction Using PCA \n",
    "\n",
    "Here we will very briefly show an application of PCA:  \n",
    "We will use your trained PCA model to reconstruct de-noised images.  \n",
    "In the cell below we plot the first 40 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plot_digits(noisy_test[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Please use the PCA model you created and fitted above.  \n",
    "1) If you called your PCA model say `pca`, than apply `pca.transform` to the test data.  \n",
    "2) Apply `pca.inverse_transform` to this result and store in a variable called `filtered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Denoising - Results:\n",
    "\n",
    "If all has been implemented correctly then in the cell below you should see the same test images, but this time with much of the noise removed.  \n",
    "Remember that the model was fitted to only the training data and had not seen these test images.\n",
    "\n",
    "__Question:__  \n",
    "Why do you think the de-noised images are blurry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plot_digits(filtered[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoder\n",
    "The code below comes from the excellent Keras blog https://blog.keras.io/. \n",
    "\n",
    "We have applied the code to the images which we have been working with in this laboratory.\n",
    "\n",
    "The code takes a little time to run, but will create noisy training images, train a de-noising convolutional autoencoder on the training images, and then display a few noisy test images as well as their denoised versions.\n",
    "\n",
    "This is simply for demonstration purposes to show that there are more powerful denoising models. \n",
    "\n",
    "__Question:__  \n",
    "Auto-encoders have some relationships to PCA, but are more general for a particular reason. What is this reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "#make autoencoder\n",
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (7, 7, 32)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "#create noisy data\n",
    "x_train, y_train, x_test, y_test = X_train_orig, y_train_orig, X_test_orig, y_test_orig\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "#Train autoencoder\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=5,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, x_test))\n",
    "\n",
    "#predict on noisy test set\n",
    "decoded_imgs = autoencoder.predict(x_test_noisy)\n",
    "\n",
    "#plot the results\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(1,2*n+1):\n",
    "    ax = plt.subplot(2, 10, i)\n",
    "    \n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    if i>10:\n",
    "        plt.imshow(decoded_imgs[i-10].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised Learning Via Clustering\n",
    "\n",
    "We will now introduce the problem of classification of the data when we have very few labelled examples.  \n",
    "\n",
    "It can often be the case that the good labelled examples are difficult to gather, due to time or cost or both.\n",
    "\n",
    "In this case the problem setting is one where we only have the resources to label 100 of the 60,000 training examples. However we still need to predict on the entire 10,000 test set.\n",
    "\n",
    "The cell below shows the first 40 training digits.   \n",
    "(We have introduced a variable called `n_labelled` and set this equal to 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "X_train, y_train, X_test, y_test = X_train_orig, y_train_orig, X_test_orig, y_test_orig\n",
    "X_train = X_train.reshape(-1,784)\n",
    "X_test = X_test.reshape(-1,784)\n",
    "n_labeled = 100\n",
    "\n",
    "plot_digits(X_train[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Fit a Logistic Regression model on the first 100 training examples.  \n",
    "Create the usual variable `y_pred` containing your model predictions on the test set, `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results - 100 Labelled Examples:\n",
    "In the cell below the accuracy and a confusion matrix of the results are displayed.\n",
    "You should find that the results are quite poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "print_results(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Means - Representative Labels:\n",
    "\n",
    "Not all training examples are necessarily equal. If we only have the resources to label 100 training examples then perhaps we should be careful about the ones we choose.\n",
    "\n",
    "We will use an unsupervised learning method, $k$-means, to cluster our training data into 100 clusters. Every training digit will thus have 10 different distances to the centroids of these clusters.\n",
    "\n",
    "For each cluster across all training images we will choose the image that is closest to the centroid of that cluster, we call these representative digits. These 100 digits are then labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "k = 100\n",
    "\n",
    "kmeans  =  KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "rep_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
    "X_rep_digits = X_train[rep_digit_idx]\n",
    "y_rep_digits = y_train[rep_digit_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representative Training Digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plot_digits(X_rep_digits[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "The new training data is stored in two variables: `X_rep_digits` and `y_rep_digits`.   \n",
    "Fit a Logistic Regression model to this data and test on `X_test`. Store the results in `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representative Training Digits - Results:\n",
    "We have given the accuracy on the test set as well as the confusion matrix.   \n",
    "We are still only using 100 training examples, but you should see the accuracy on the test set increase significantly.\n",
    "\n",
    "__Question:__   \n",
    "This technique worked in this particular case, but can you think of cases where this will not be of benefit or may even be detrimental?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "print_results(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagating Cluster Labels:\n",
    "(The code from the cell below comes from Geron's Dimensionality Reduction and Clustering work).\n",
    "\n",
    "We have just used a small number of labelled digits and then labelled our clusters according to their distance from the representative labels given by these labelled digits. Thus we are hoping that the representative labels are good labels and that $k$-means provides a good clustering of that label.\n",
    "\n",
    "Now if we throw out the furthest 5% (i.e. the least representative labels) then we gain another 5% increase in accuracy on the test set.\n",
    "\n",
    "Thus, with only labelled 100 training examples, using $k$-means clustering to generate representative images which are then used to propagate labels, we are able to increase our test accuracy to over 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL - for understanding only\n",
    "y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_==i] = y_rep_digits[i]\n",
    "\n",
    "percentile_closest = 95\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "\n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
    "\n",
    "log_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
    "\n",
    "y_pred = log_reg.predict(X_test.reshape(-1,784))\n",
    "print_results(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
